{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from tqdm import tdqm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "print(torch.backends.mps.is_available())\n",
    "mps_device = torch.device(\"mps\")\n",
    "print(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random \n",
    "import pandas as pd\n",
    "# open method used to open different extension image file\n",
    "TRAIN_DATA_FOLDER = \"../datasets/MRI/data/mri_data_full/train\"\n",
    "train_tumor_filenames = os.listdir(f'{TRAIN_DATA_FOLDER}/tumor/')\n",
    "num_train_tumor_files = len(train_tumor_filenames)\n",
    "print(\"Number of Training Tumor Files: \", num_train_tumor_files)\n",
    "\n",
    "train_notumor_filenames = os.listdir(f'{TRAIN_DATA_FOLDER}/notumor/')\n",
    "num_train_notumor_files = len(train_notumor_filenames)\n",
    "print(\"Number of Training No Tumor Files: \", num_train_notumor_files)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model Architecture\n",
    "![Transformer Model](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tA7xE2dQA_dfzA0Bub5TVw.png)\n",
    "\n",
    "## Resizing Image\n",
    "\n",
    "### Formula [e.g (N,1,28,28)] ![image.png](attachment:image.png)\n",
    "![Reshaping Image](https://miro.medium.com/v2/resize:fit:822/format:webp/1*CFbOxEuvo-Pgq7ETIrt0Eg.png)\n",
    "\n",
    "\n",
    "### Positional Embedding\n",
    "![Positional Embedding Formula](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpRYHE0XjVkxRVKFrWkzuw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import ToPILImage\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     IMAGE_SIZE = (1,128,128)\n",
    "#     NUM_PATCHES = 32\n",
    "\n",
    "#     train_dataset = MRIDataset('../datasets/MRI/data/train/', transform=ToTensor(),image_shape = IMAGE_SIZE[1])\n",
    "#     train_loader  = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "#     test_dataset = MRIDataset('../datasets/MRI/test/', transform=ToTensor(),image_shape = IMAGE_SIZE[1])\n",
    "#     test_loader  = DataLoader(test_dataset, shuffle=False, batch_size=128)\n",
    "\n",
    "#     # Get one batch (single image and label)\n",
    "#     for images, labels in train_loader:\n",
    "#         # Extract the first image and label\n",
    "#         image = images[0]  # Tensor shape: (C, H, W)\n",
    "#         label = labels[0]\n",
    "        \n",
    "#         if label == 1:\n",
    "#             # Convert the tensor back to a PIL image for display\n",
    "#             to_pil = ToPILImage()\n",
    "#             image_pil = to_pil(image)\n",
    "\n",
    "#             # Display the image\n",
    "#             plt.imshow(image_pil)\n",
    "#             plt.title(f\"Label: {label}\")\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "#             break\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         # Extract the first image and label\n",
    "#         image = images[0]  # Tensor shape: (C, H, W)\n",
    "#         label = labels[0]\n",
    "\n",
    "#         if label == 0:\n",
    "#             # Convert the tensor back to a PIL image for display\n",
    "#             to_pil = ToPILImage()\n",
    "#             image_pil = to_pil(image)\n",
    "\n",
    "#             # Display the image\n",
    "#             plt.imshow(image_pil)\n",
    "#             plt.title(f\"Label: {label}\")\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "#             break\n",
    "\n",
    "#     print(train_loader)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "#     # print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available else \"\")\n",
    "#     model = MyViT(IMAGE_SIZE, n_patches=NUM_PATCHES, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "\n",
    "#     N_EPOCHS = 5\n",
    "#     LR = 0.005\n",
    "    \n",
    "#     # Training Loop\n",
    "#     optimizer = Adam(model.parameters(),lr = LR)\n",
    "#     scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "#     criterion = CrossEntropyLoss()\n",
    "#     for epoch in trange(N_EPOCHS,desc=\"Training\"):\n",
    "#         train_loss = 0.0\n",
    "#         for batch in tqdm(train_loader,desc=f\"Epoch{epoch+1} in training\", leave=False):\n",
    "#             x,y = batch\n",
    "#             # print(y.shape)\n",
    "#             x,y = x.to(device),y.to(device)\n",
    "#             y_hat = model(x)\n",
    "#             loss = criterion(y_hat,y)\n",
    "#             train_loss += loss.detach().cpu().item()/len(train_loader)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         print(f\"Epoch {epoch+1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         correct,total = 0,0\n",
    "#         test_loss = 0.0\n",
    "#         for batch in tqdm(test_loader,desc=\"Testing\"):\n",
    "#             x,y = batch\n",
    "#             x,y = x.to(device),y.to(device)\n",
    "#             y_hat = model(x)\n",
    "#             loss = criterion(y_hat,y)\n",
    "#             test_loss += loss.detach().cpu().item()/len(test_loader)\n",
    "\n",
    "#             correct += torch.sum(torch.argmax(y_hat,dim=1) == y).detach().cpu()\n",
    "#             total += len(x)\n",
    "#         print(f\"Test loss: {test_loss:.2f}\")\n",
    "#         print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Self Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MyViT.ViT import * \n",
    "# from MyViT.utils import MRIDataset,get_positional_embeddings,patchify\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal AI Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing Causal Variational Encoder Behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import ToTensor\n",
    "# from CausalVAE.causal_vae import CausalVAE\n",
    "# from MyViT.utils import MRIDataset  # Assuming MRIDataset is defined in your utils.py\n",
    "\n",
    "# # Assume IMAGE_SIZE is defined somewhere in your code; for example:\n",
    "# # IMAGE_SIZE = (1, 224, 224)  -> grayscale image with height and width of 224 pixels\n",
    "# image_size = 224  # You can set this based on IMAGE_SIZE[1]\n",
    "# input_dim = image_size * image_size  # For grayscale images\n",
    "\n",
    "# # Define the dimensions for the causal VAE\n",
    "# z_dim = 50\n",
    "# hidden_dim = 100\n",
    "\n",
    "# # Instantiate the causal VAE model\n",
    "# model = CausalVAE(input_dim=input_dim, z_dim=z_dim, hidden_dim=hidden_dim)\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# # Load your MRI dataset using MRIDataset from utils.py\n",
    "# train_dataset = MRIDataset('../datasets/MRI/data/train/', transform=ToTensor(), image_shape=image_size)\n",
    "# train_loader  = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "# # Get a random batch from the train_loader\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "# # Assuming images are of shape (batch_size, channels, height, width)\n",
    "# # If the images are grayscale, channels=1; we flatten the images for the VAE\n",
    "# batch_size = images.size(0)\n",
    "# images_flat = images.view(batch_size, -1)  # shape: (batch_size, input_dim)\n",
    "\n",
    "# # Pass the images through the causal VAE to obtain reconstructions and latent variables\n",
    "# with torch.no_grad():\n",
    "#     reconstructions, z, z_mu, z_logvar = model(images_flat)\n",
    "\n",
    "# # Helper function to display images\n",
    "# def show_images(imgs, title, image_shape=(image_size, image_size), n=4):\n",
    "#     # Only use the first n images\n",
    "#     imgs = imgs[:n]\n",
    "#     imgs = imgs.view(-1, *image_shape).cpu()\n",
    "#     batch = imgs.size(0)\n",
    "#     fig, axes = plt.subplots(1, batch, figsize=(3 * batch, 3))\n",
    "#     for i in range(batch):\n",
    "#         axes[i].imshow(imgs[i], cmap='gray')\n",
    "#         axes[i].axis('off')\n",
    "#     plt.suptitle(title)\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize the original and reconstructed images\n",
    "# show_images(images_flat, \"Original MRI Images\", image_shape=(image_size, image_size))\n",
    "# show_images(reconstructions, \"Reconstructed MRI Images\", image_shape=(image_size, image_size))\n",
    "\n",
    "# import torch\n",
    "# from CausalTransformer.causal_transformer import HybridModel\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define parameters for MyViT\n",
    "#     vit_params = {\n",
    "#         'chw': (1, 224, 224),  # For example, grayscale images of 224x224\n",
    "#         'n_patches': 8,\n",
    "#         'n_blocks': 2,\n",
    "#         'hidden_d': 64,        # Increased hidden dimension for better representation\n",
    "#         'n_heads': 2,\n",
    "#         'out_d': 10            # This is not used in the hybrid model since we fuse later\n",
    "#     }\n",
    "#     # For the VAE, if using raw images of 224x224:\n",
    "#     vae_input_dim = 224 * 224\n",
    "\n",
    "#     # Instantiate the hybrid model\n",
    "#     model = HybridModel(vit_params, vae_input_dim, vae_z_dim=50, vae_hidden_dim=400, num_classes=2)\n",
    "#     # Example input: a batch of MRI images (batch_size, channels, height, width)\n",
    "#     dummy_images = torch.randn(4, 1, 224, 224)\n",
    "#     outputs = model(dummy_images, return_all=True)\n",
    "#     print(\"Hybrid Model Output:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training, Evaluation, and Analysis\n",
    "\n",
    "1. Define the Loss Functions\n",
    "\n",
    "We have three components in our loss:\n",
    "\t•\tClassification Loss: Measures how well the model predicts tumor vs. non-tumor. We’ll use Cross Entropy Loss.\n",
    "\t•\tReconstruction Loss: Measures how close the VAE’s reconstruction is to the original image. For images normalized between 0 and 1, Binary Cross Entropy (BCE) works well.\n",
    "\t•\tKL Divergence Loss: Regularizes the latent distribution  z  to be close to a standard normal distribution.\n",
    "\n",
    "The total loss is given by:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Total Loss} = \\text{Classification Loss} + \\lambda \\times (\\text{Reconstruction Loss} + \\text{KL Divergence Loss})\n",
    "$$\n",
    "\n",
    "\n",
    "where $$\\lambda$$ is a hyperparameter controlling the weight of the VAE losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# img = Image.open(\"../datasets/MRI/data/train/tumor/Tr-me_0010.jpg\")  # path to your file\n",
    "# print(img.size)  # returns (width, height) in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from tqdm import tqdm, trange\n",
    "from MyViT.utils import MRIDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from CausalVAE.causal_vae import CausalVAE\n",
    "from CausalTransformer.causal_transformer import HybridModel\n",
    "import matplotlib.pyplot as plt\n",
    "from MyViT.utils import MRIDataset  # Assuming MRIDataset is defined in your utils.py\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = torch.device(\"cpu\" if torch.cuda.is_available() else \"mps\")\n",
    "print(\"Using device: \", device)\n",
    "# Define the image size constant (e.g., 224 for 224x224 images)\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MRIDataset('../datasets/MRI/data/train/', transform=ToTensor(),image_shape = IMAGE_SIZE)\n",
    "train_loader  = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE,num_workers=0)\n",
    "\n",
    "test_dataset = MRIDataset('../datasets/MRI/data/test/', transform=ToTensor(),image_shape = IMAGE_SIZE)\n",
    "test_loader  = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE,num_workers=0)\n",
    "\n",
    "\n",
    "# --- Step 1: Define Loss Functions and Hyperparameters ---\n",
    "\n",
    "\n",
    " \n",
    "def kl_divergence_loss(z_mu, z_logvar):\n",
    "    return -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp())\n",
    "\n",
    "N_EPOCHS = 50\n",
    "lambda_vae = 0.01\n",
    "learning_rate = 1e-2\n",
    "\n",
    "NUM_PATCHES = 32\n",
    "N_BLOCKS = 2\n",
    "HIDDEN_D = 64\n",
    "N_HEADS = 2\n",
    "OUT_D = 10\n",
    "# --- Step 2: Instantiate Your Hybrid Model & Optimizer ---\n",
    "\n",
    "# Example ViT config using IMAGE_SIZE\n",
    "vit_config = {\n",
    "    'chw': (1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    'n_patches': NUM_PATCHES,\n",
    "    'n_blocks': N_BLOCKS,\n",
    "    'hidden_d': HIDDEN_D,\n",
    "    'n_heads': N_HEADS,\n",
    "    'out_d': OUT_D  # Not used in final classification as we override with fusion_layer\n",
    "}\n",
    "\n",
    "vae_input_dim = IMAGE_SIZE * IMAGE_SIZE  # For grayscale images\n",
    "\n",
    "# Instantiate your HybridModel (assumed to be imported)\n",
    "model = HybridModel(\n",
    "    vit_config,\n",
    "    vae_input_dim=vae_input_dim,\n",
    "    vae_z_dim=50,\n",
    "    vae_hidden_dim=400,\n",
    "    num_classes=2,\n",
    "    batch_size = BATCH_SIZE\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "classification_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "reconstruction_loss_fn = torch.nn.BCELoss(reduction='mean')\n",
    "   \n",
    "# --- Step 3: Define an Accuracy Evaluation Function ---\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    correct_total = 0\n",
    "    samples_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # We only need logits for accuracy\n",
    "            logits = model(images)\n",
    "            _, predicted_classes = torch.max(logits, dim=1)\n",
    "\n",
    "            correct_total += (predicted_classes == labels).sum().item()\n",
    "            samples_total += labels.size(0)\n",
    "\n",
    "    return correct_total / samples_total if samples_total > 0 else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train dataset size: 2934\n",
      "Test dataset size: 711\n",
      "Batch shape: torch.Size([32, 1, 256, 256]), Labels: tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1])\n",
      "Testing forward pass...\n",
      "Device: cpu | Logits Shape: torch.Size([32, 2]), Reconstructed Shape: torch.Size([32, 65536]) | Time Taken: 0.584326982498169\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def test_function(train_dataset,test_dataset,train_loader,model,Device):\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "    x, y = next(iter(train_loader))  # Get a single batch\n",
    "    x, y = x.to(Device), y.to(Device)\n",
    "    print(f\"Batch shape: {x.shape}, Labels: {y}\")\n",
    "    print(\"Testing forward pass...\")\n",
    "    try:\n",
    "        time_start = time.time()\n",
    "        logits, x_recon, z_mu, z_logvar = model(x, return_all=True)\n",
    "        total_time = time.time()-time_start \n",
    "        print(f\"Device: {Device} | Logits Shape: {logits.shape}, Reconstructed Shape: {x_recon.shape} | Time Taken: {total_time}\")\n",
    "    except Exception as e:\n",
    "        print(\"Forward pass failed:\", str(e))\n",
    "\n",
    "test_function(train_dataset,test_dataset,train_loader,model,Device=device)\n",
    "# test_function(train_loader,model.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m images_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruction_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_recon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# KL Divergence Loss\u001b[39;00m\n\u001b[1;32m     28\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m kl_divergence_loss(z_mu, z_logvar)\n",
      "File \u001b[0;32m~/miniconda3/envs/TumorDetection/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TumorDetection/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/TumorDetection/lib/python3.12/site-packages/torch/nn/modules/loss.py:697\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TumorDetection/lib/python3.12/site-packages/torch/nn/functional.py:3554\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3551\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3552\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 4: Training Loop with Periodic Accuracy Checks ---\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "model.train()\n",
    "for epoch in trange(N_EPOCHS,desc=\"Training\"):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader,desc=f\"Epoch{epoch+1} in training\", leave=False):\n",
    "        \n",
    "        x,y = batch\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass, returning VAE outputs for reconstruction & KL losses\n",
    "        logits, x_recon, z_mu, z_logvar = model(x, return_all=True)\n",
    "        # Classification Loss\n",
    "        class_loss = classification_loss_fn(logits, y.long())\n",
    "\n",
    "    \n",
    "        # Reconstruction Loss: flatten images using IMAGE_SIZE\n",
    "        batch_size = x.size(0)\n",
    "        images_flat = x.view(batch_size, -1)\n",
    "        recon_loss = reconstruction_loss_fn(x_recon, images_flat)\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = kl_divergence_loss(z_mu, z_logvar)\n",
    "\n",
    "        # Combined Loss\n",
    "        total_loss = class_loss + lambda_vae * (recon_loss + kl_loss)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "    scheduler.step()\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "\n",
    "    # Evaluate accuracy on the training set\n",
    "    train_acc = evaluate_accuracy(model, train_loader,device='mps')\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{N_EPOCHS}] | Loss: {avg_epoch_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    if train_acc > 0.95:\n",
    "        break\n",
    "# --- Optional: Plot Training Loss and Accuracy ---\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save your model weights:\n",
    "torch.save(model.state_dict(), \"hybrid_model_weights.pth\")\n",
    "\n",
    "\n",
    "NUM_PATCHES = 16\n",
    "N_BLOCKS = 2\n",
    "HIDDEN_D = 8\n",
    "N_HEADS = 2\n",
    "OUT_D = 10\n",
    "# --- Step 2: Instantiate Your Hybrid Model & Optimizer ---\n",
    "\n",
    "# Example ViT config using IMAGE_SIZE\n",
    "vit_config = {\n",
    "    'chw': (1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    'n_patches': NUM_PATCHES,\n",
    "    'n_blocks': N_BLOCKS,\n",
    "    'hidden_d': HIDDEN_D,\n",
    "    'n_heads': N_HEADS,\n",
    "    'out_d': OUT_D  # Not used in final classification as we override with fusion_layer\n",
    "}\n",
    "\n",
    "vae_input_dim = IMAGE_SIZE * IMAGE_SIZE  # For grayscale images\n",
    "\n",
    "# Instantiate your HybridModel (assumed to be imported)\n",
    "model = HybridModel(\n",
    "    vit_config,\n",
    "    vae_input_dim=vae_input_dim,\n",
    "    vae_z_dim=50,\n",
    "    vae_hidden_dim=400,\n",
    "    num_classes=2,\n",
    "    batch_size = BATCH_SIZE\n",
    ").to(device)\n",
    "# Later, to load these weights:\n",
    "# model = HybridModel(vit_config, vae_input_dim, vae_z_dim=50, vae_hidden_dim=400, num_classes=2)\n",
    "model.load_state_dict(torch.load(\"hybrid_model_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = evaluate_accuracy(model, test_loader,device='cpu')\n",
    "print(test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Visualize Reconstructions Using IMAGE_SIZE ---\n",
    "\n",
    "def show_images(imgs, title,labels, image_shape=(IMAGE_SIZE, IMAGE_SIZE), n=4):\n",
    "    imgs = imgs[:n]\n",
    "    imgs = imgs.view(-1, *image_shape).cpu()\n",
    "    batch = imgs.size(0)\n",
    "    labels = labels[:n]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, batch, figsize=(3 * batch, 3))\n",
    "    for i in range(batch):\n",
    "        axes[i].imshow(imgs[i], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].title.set_text(f\"{\"Tumor : 1\" if labels[i].item() == 1 else \"No Tumor : 0\"}\")\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch from the train_loader for visualization\n",
    "data_iter = iter(test_loader)\n",
    "x, y = next(data_iter)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "batch_size = x.size(0)\n",
    "images_flat = x.view(batch_size, -1)\n",
    "\n",
    "logits,_,_,_ = model(x,return_all=True)\n",
    "predicted = torch.max(logits, dim=1)\n",
    "# torch.cat(predicted.indices)\n",
    "with torch.no_grad():\n",
    "    x_recon, _,_, _= model.causal_vae(images_flat)\n",
    "\n",
    "print(f\"Image Size: {IMAGE_SIZE}\")\n",
    "show_images(images_flat, \"Original MRI Images\",labels = y, image_shape=(IMAGE_SIZE, IMAGE_SIZE), n=10)\n",
    "show_images(x_recon, \"Reconstructed MRI Images\", labels = predicted.indices ,image_shape=(IMAGE_SIZE, IMAGE_SIZE), n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TumorDetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
